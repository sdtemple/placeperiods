period_placer.h5
------
Final Model Layers Description:
1 3-dimensional convolutional layer that convolves one word at a time with 16 filters
1 fully-connected layer with 12 nodes and rectified linear unit activation
1 fully-connected layer with 1 node and sigmoid activation

Final Model Parameters:
RMSprop optimizer
binary_crossentropy loss
No dropout
10 epochs
30 batch_size

Final Model Training Data:
64 college and HS essays
------

Model Building Notes.

It isn't hard to predict no period. Between most instances of the sliding window, there is no period.
We want a NN that does not predict false positives (incorrect full stops).
We want a NN that predicts more true positives than false positives (get most periods right).
We look at precision and recall in the confusion matrix to evaluate model performance.

Shallow convolution neural nets predict periods well.
I built models with high precision and recall with one convolutional layer and one fully-connected layer.

Models trained on formal essays perform well at predicting periods in informal speech.
More text data for training gives big improvements in prediction.

Talapas GPUs used to grid search for hyper parameters.
Keras offer a big toolbox of hyper parameters:
	Network topology (# filters, # outputs, # nodes, # layers)
	CNNs
	Optimizers (learning rate, decay, momentum)
	Kernel initialization
	Dropout layers (helps reduce overfitting to training data and coadaptation of nodes)
	Batch sizes (how much you read before you make a judgment)
	Epochs (how many times you read an essay)
	Activations (threshold function for neurons)
	Padding for CNNs (I do not use zero padding)
Hyper parameters I played with:
	Network topology
	CNNs
	Optimizers
	Batch sizes
	Epochs
I consider convolutional filters that correspond to filtering either one word at a time one n-gram at a time.
Code developed in MakeNGramSplitWindows to allow for any n for n-grams and for random word2vec representations for words not in the Google word2vec model.
I only consider 3-grams and a zero vector representation for words foreign to the Google word2vec model.
The activation in the final layer should be 'sigmoid'.
AveragePooling and MaxPooling layers degrade model performance.

Google's word2vec representation doesn't contain some words like AND and OR.
Google's word2vec representation doesn't contain foreign words like Schmetterling.
Google's word2vec representation doesn't contain numbers. Processing files turn all numbers into string "number".
Google's word2Vec representation does contain contractions.

Consider further training of model with text data from Uppsala Student English Corpus.
Consider removing filler words like uh, um, and ok.